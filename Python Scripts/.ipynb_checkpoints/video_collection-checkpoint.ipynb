{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c529f331-5daf-4ca5-85aa-cf9b2341bba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def get_channel_details(api_key, channel_id):\n",
    "    url = f\"https://www.googleapis.com/youtube/v3/channels?part=statistics&id={channel_id}&key={api_key}\"\n",
    "    response = requests.get(url).json()\n",
    "    if 'items' in response and len(response['items']) > 0:\n",
    "        stats = response['items'][0]['statistics']\n",
    "        return {\n",
    "            'subscriber_count': stats.get('subscriberCount', '0')\n",
    "        }\n",
    "    return {'subscriber_count': '0'}\n",
    "\n",
    "def get_video_details(api_key, video_id):\n",
    "    url = f\"https://www.googleapis.com/youtube/v3/videos?part=statistics,snippet,contentDetails&id={video_id}&key={api_key}\"\n",
    "    response = requests.get(url).json()\n",
    "    details = {}\n",
    "    if 'items' in response and len(response['items']) > 0:\n",
    "        stats = response['items'][0]['statistics']\n",
    "        snippet = response['items'][0]['snippet']\n",
    "        content_details = response['items'][0]['contentDetails']\n",
    "        duration = parse_duration(content_details['duration'])\n",
    "        details = {\n",
    "            'description': snippet.get('description', ''),\n",
    "            'channel_id': snippet['channelId'],\n",
    "            'likes': stats.get('likeCount', '0'),\n",
    "            'views': stats.get('viewCount', '0'),\n",
    "            'comments': stats.get('commentCount', '0'),\n",
    "            'stages': snippet.get('liveBroadcastContent', ''),\n",
    "            'category': snippet.get('categoryId', ''),\n",
    "            'licensed_content': content_details.get('licensedContent', False),\n",
    "            'duration': duration,\n",
    "            'comments_enabled': 'commentCount' in stats\n",
    "        }\n",
    "    return details\n",
    "\n",
    "def parse_duration(duration):\n",
    "    import isodate\n",
    "    duration = isodate.parse_duration(duration)\n",
    "    return duration.total_seconds() / 60  # Convert to minutes\n",
    "\n",
    "def get_top_comments(api_key, video_id):\n",
    "    url = f\"https://www.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId={video_id}&key={api_key}&order=relevance&maxResults=3\"\n",
    "    response = requests.get(url).json()\n",
    "    top_comments = []\n",
    "    if 'items' in response:\n",
    "        for item in response['items']:\n",
    "            top_comments.append(item['snippet']['topLevelComment']['snippet']['textDisplay'])\n",
    "    return top_comments\n",
    "\n",
    "def collect_videos(api_key, search_query, order, max_videos):\n",
    "    collected_data = []\n",
    "    next_page_token = None\n",
    "    while len(collected_data) < max_videos:\n",
    "        url = f\"https://www.googleapis.com/youtube/v3/search?key={api_key}&q={search_query}&part=snippet&type=video&maxResults={min(50, max_videos-len(collected_data))}&order={order}&pageToken={next_page_token or ''}\"\n",
    "        response = requests.get(url).json()\n",
    "        if \"items\" not in response:\n",
    "            break\n",
    "        \n",
    "        for item in response[\"items\"]:\n",
    "            video_id = item[\"id\"][\"videoId\"]\n",
    "            video_details = get_video_details(api_key, video_id)\n",
    "            duration = video_details.get('duration', 0)\n",
    "            if 4 <= duration <= 20:\n",
    "                channel_details = get_channel_details(api_key, video_details['channel_id'])\n",
    "                top_comments = get_top_comments(api_key, video_id)\n",
    "                \n",
    "                collected_data.append({\n",
    "                    \"search_query\": search_query,\n",
    "                    \"video_id\": video_id,\n",
    "                    \"title\": item[\"snippet\"][\"title\"],\n",
    "                    \"video_url\": f\"https://www.youtube.com/watch?v={video_id}\",\n",
    "                    \"description\": video_details['description'],\n",
    "                    \"channel_id\": video_details['channel_id'],\n",
    "                    \"subscriber_count\": channel_details['subscriber_count'],\n",
    "                    \"likes\": video_details['likes'],\n",
    "                    \"views\": video_details['views'],\n",
    "                    \"comments\": video_details['comments'],\n",
    "                    \"top_comments\": top_comments,\n",
    "                    \"stages\": video_details['stages'],\n",
    "                    \"category\": video_details['category'],\n",
    "                    \"licensed_content\": video_details['licensed_content'],\n",
    "                    \"duration\": video_details['duration'],\n",
    "                    \"comments_enabled\": video_details['comments_enabled'],\n",
    "                    \"order\": order\n",
    "                })\n",
    "                \n",
    "                if len(collected_data) >= max_videos:\n",
    "                    break\n",
    "        \n",
    "        next_page_token = response.get(\"nextPageToken\")\n",
    "        if not next_page_token:\n",
    "            break\n",
    "    \n",
    "    return collected_data\n",
    "\n",
    "# Your API Key\n",
    "api_key = \"AIzaSyDUx6ksTMOX5PdEJccfUdRj5MsG_AcySYI\"\n",
    "\n",
    "# Define a list of search queries\n",
    "search_queries = [\"metoo my story\"]\n",
    "videos_per_query = 200  # Total videos per category\n",
    "videos_per_type = 100  # Number of videos per relevance and recent type\n",
    "\n",
    "# Placeholder for collected video data\n",
    "video_data = []\n",
    "\n",
    "for search_query in search_queries:\n",
    "    # Collect relevance videos\n",
    "    video_data += collect_videos(api_key, search_query, 'relevance', videos_per_type)\n",
    "    # Collect recent videos\n",
    "    video_data += collect_videos(api_key, search_query, 'date', videos_per_type)\n",
    "\n",
    "# Save video data to a CSV file\n",
    "df = pd.DataFrame(video_data)\n",
    "df.to_csv('metoo_my_story_videos.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a590e4d4-e7e8-4d67-bc81-0c482c6f7c78",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m webdriver\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mby\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m By\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def scrape_tiktok(search_query, max_videos):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(f\"https://www.tiktok.com/search?q={search_query}\")\n",
    "    time.sleep(5)  # Adjust as necessary\n",
    "\n",
    "    videos = []\n",
    "    while len(videos) < max_videos:\n",
    "        # Find video elements\n",
    "        video_elements = driver.find_elements(By.XPATH, \"//div[@class='video-feed-item-wrapper']\")\n",
    "        for elem in video_elements:\n",
    "            if len(videos) >= max_videos:\n",
    "                break\n",
    "            title = elem.find_element(By.XPATH, \".//a\").text\n",
    "            video_url = elem.find_element(By.XPATH, \".//a\").get_attribute(\"href\")\n",
    "            likes = elem.find_element(By.XPATH, \".//span[@class='like-count']\").text\n",
    "            views = elem.find_element(By.XPATH, \".//span[@class='play-count']\").text\n",
    "            videos.append({\n",
    "                \"title\": title,\n",
    "                \"video_url\": video_url,\n",
    "                \"likes\": likes,\n",
    "                \"views\": views,\n",
    "                \"search_query\": search_query,\n",
    "                \"platform\": \"TikTok\"\n",
    "            })\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)  # Adjust as necessary\n",
    "\n",
    "    driver.quit()\n",
    "    return videos\n",
    "\n",
    "def scrape_instagram(search_query, max_videos):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(f\"https://www.instagram.com/explore/tags/{search_query}/\")\n",
    "    time.sleep(5)  # Adjust as necessary\n",
    "\n",
    "    videos = []\n",
    "    while len(videos) < max_videos:\n",
    "        # Find reel elements\n",
    "        reel_elements = driver.find_elements(By.XPATH, \"//div[@class='v1Nh3 kIKUG  _bz0w']\")\n",
    "        for elem in reel_elements:\n",
    "            if len(videos) >= max_videos:\n",
    "                break\n",
    "            video_url = elem.find_element(By.XPATH, \".//a\").get_attribute(\"href\")\n",
    "            elem.click()\n",
    "            time.sleep(3)  # Adjust as necessary\n",
    "            likes = driver.find_element(By.XPATH, \"//div[@class='Nm9Fw']/button/span\").text\n",
    "            views = driver.find_element(By.XPATH, \"//span[@class='vcOH2']\").text\n",
    "            title = driver.find_element(By.XPATH, \"//div[@class='C4VMK']/span\").text\n",
    "            videos.append({\n",
    "                \"title\": title,\n",
    "                \"video_url\": video_url,\n",
    "                \"likes\": likes,\n",
    "                \"views\": views,\n",
    "                \"search_query\": search_query,\n",
    "                \"platform\": \"Instagram\"\n",
    "            })\n",
    "            driver.back()\n",
    "            time.sleep(2)  # Adjust as necessary\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)  # Adjust as necessary\n",
    "\n",
    "    driver.quit()\n",
    "    return videos\n",
    "\n",
    "# Define search query and video count\n",
    "search_query = \"black women disparities medicine\"\n",
    "videos_per_platform = 50  # 100 videos total, 50 from TikTok and 50 from Instagram\n",
    "\n",
    "# Scrape TikTok videos\n",
    "tiktok_videos = scrape_tiktok(search_query, videos_per_platform)\n",
    "\n",
    "# Scrape Instagram Reels\n",
    "instagram_videos = scrape_instagram(search_query, videos_per_platform)\n",
    "\n",
    "# Combine data and save to CSV\n",
    "all_videos = tiktok_videos + instagram_videos\n",
    "df = pd.DataFrame(all_videos)\n",
    "df.to_csv('black_women_disparities_medicine_videos.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e97c8170-6d37-4c7f-9c20-451170bc6e75",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'results'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 53\u001b[0m\n\u001b[0;32m     50\u001b[0m videos_per_platform \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m  \u001b[38;5;66;03m# 100 videos total, 50 from TikTok and 50 from Instagram\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Get TikTok videos\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m tiktok_videos \u001b[38;5;241m=\u001b[39m get_tiktok_data(search_query, videos_per_platform)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Get Instagram Reels\u001b[39;00m\n\u001b[0;32m     56\u001b[0m instagram_videos \u001b[38;5;241m=\u001b[39m get_instagram_data(search_query, videos_per_platform)\n",
      "Cell \u001b[1;32mIn[4], line 13\u001b[0m, in \u001b[0;36mget_tiktok_data\u001b[1;34m(search_query, max_videos)\u001b[0m\n\u001b[0;32m     11\u001b[0m data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m     12\u001b[0m videos \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     14\u001b[0m     video \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m: item\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo_url\u001b[39m\u001b[38;5;124m\"\u001b[39m: item\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo_url\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplatform\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTikTok\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     21\u001b[0m     }\n\u001b[0;32m     22\u001b[0m     videos\u001b[38;5;241m.\u001b[39mappend(video)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'results'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def get_tiktok_data(search_query, max_videos):\n",
    "    api_url = \"https://www.tiktok.com/api/search\"  # Example URL, might not be accurate\n",
    "    params = {\n",
    "        'query': search_query,\n",
    "        'limit': max_videos\n",
    "    }\n",
    "    response = requests.get(api_url, params=params)\n",
    "    data = response.json()\n",
    "    videos = []\n",
    "    for item in data['results']:\n",
    "        video = {\n",
    "            \"title\": item.get('title', ''),\n",
    "            \"video_url\": item.get('video_url', ''),\n",
    "            \"likes\": item.get('likes', 0),\n",
    "            \"views\": item.get('views', 0),\n",
    "            \"search_query\": search_query,\n",
    "            \"platform\": \"TikTok\"\n",
    "        }\n",
    "        videos.append(video)\n",
    "    return videos\n",
    "\n",
    "def get_instagram_data(search_query, max_videos):\n",
    "    access_token = \"YOUR_INSTAGRAM_ACCESS_TOKEN\"  # You need to get this token from Instagram\n",
    "    api_url = f\"https://graph.instagram.com/v11.0/search\"\n",
    "    params = {\n",
    "        'q': search_query,\n",
    "        'access_token': access_token,\n",
    "        'limit': max_videos\n",
    "    }\n",
    "    response = requests.get(api_url, params=params)\n",
    "    data = response.json()\n",
    "    videos = []\n",
    "    for item in data['data']:\n",
    "        video = {\n",
    "            \"title\": item.get('caption', ''),\n",
    "            \"video_url\": item.get('permalink', ''),\n",
    "            \"likes\": item.get('like_count', 0),\n",
    "            \"views\": item.get('view_count', 0),\n",
    "            \"search_query\": search_query,\n",
    "            \"platform\": \"Instagram\"\n",
    "        }\n",
    "        videos.append(video)\n",
    "    return videos\n",
    "\n",
    "# Define search query and video count\n",
    "search_query = \"black women disparities medicine\"\n",
    "videos_per_platform = 50  # 100 videos total, 50 from TikTok and 50 from Instagram\n",
    "\n",
    "# Get TikTok videos\n",
    "tiktok_videos = get_tiktok_data(search_query, videos_per_platform)\n",
    "\n",
    "# Get Instagram Reels\n",
    "instagram_videos = get_instagram_data(search_query, videos_per_platform)\n",
    "\n",
    "# Combine data and save to CSV\n",
    "all_videos = tiktok_videos + instagram_videos\n",
    "df = pd.DataFrame(all_videos)\n",
    "df.to_csv('black_women_disparities_medicine_videos.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c499ff1-72a0-4aaa-bfa1-f3f3b325a791",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'isodate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 108\u001b[0m\n\u001b[0;32m    105\u001b[0m video_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# Collect relevance videos\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m video_data \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m collect_videos(api_key, search_query, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelevance\u001b[39m\u001b[38;5;124m'\u001b[39m, videos_per_type)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# Collect recent videos\u001b[39;00m\n\u001b[0;32m    110\u001b[0m video_data \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m collect_videos(api_key, search_query, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m, videos_per_type)\n",
      "Cell \u001b[1;32mIn[6], line 62\u001b[0m, in \u001b[0;36mcollect_videos\u001b[1;34m(api_key, search_query, order, max_videos)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m     61\u001b[0m     video_id \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideoId\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 62\u001b[0m     video_details \u001b[38;5;241m=\u001b[39m get_video_details(api_key, video_id)\n\u001b[0;32m     63\u001b[0m     duration \u001b[38;5;241m=\u001b[39m video_details\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mduration\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m duration \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m:  \u001b[38;5;66;03m# Example duration filter in minutes\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 22\u001b[0m, in \u001b[0;36mget_video_details\u001b[1;34m(api_key, video_id)\u001b[0m\n\u001b[0;32m     20\u001b[0m     snippet \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msnippet\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     21\u001b[0m     content_details \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontentDetails\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 22\u001b[0m     duration \u001b[38;5;241m=\u001b[39m parse_duration(content_details[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mduration\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     23\u001b[0m     details \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m'\u001b[39m: snippet\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     25\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannel_id\u001b[39m\u001b[38;5;124m'\u001b[39m: snippet[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannelId\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomments_enabled\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcommentCount\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m stats\n\u001b[0;32m     34\u001b[0m     }\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m details\n",
      "Cell \u001b[1;32mIn[6], line 38\u001b[0m, in \u001b[0;36mparse_duration\u001b[1;34m(duration)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_duration\u001b[39m(duration):\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01misodate\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     duration \u001b[38;5;241m=\u001b[39m isodate\u001b[38;5;241m.\u001b[39mparse_duration(duration)\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m duration\u001b[38;5;241m.\u001b[39mtotal_seconds() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m60\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'isodate'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def get_channel_details(api_key, channel_id):\n",
    "    url = f\"https://www.googleapis.com/youtube/v3/channels?part=statistics&id={channel_id}&key={api_key}\"\n",
    "    response = requests.get(url).json()\n",
    "    if 'items' in response and len(response['items']) > 0:\n",
    "        stats = response['items'][0]['statistics']\n",
    "        return {\n",
    "            'subscriber_count': stats.get('subscriberCount', '0')\n",
    "        }\n",
    "    return {'subscriber_count': '0'}\n",
    "\n",
    "def get_video_details(api_key, video_id):\n",
    "    url = f\"https://www.googleapis.com/youtube/v3/videos?part=statistics,snippet,contentDetails&id={video_id}&key={api_key}\"\n",
    "    response = requests.get(url).json()\n",
    "    details = {}\n",
    "    if 'items' in response and len(response['items']) > 0:\n",
    "        stats = response['items'][0]['statistics']\n",
    "        snippet = response['items'][0]['snippet']\n",
    "        content_details = response['items'][0]['contentDetails']\n",
    "        duration = parse_duration(content_details['duration'])\n",
    "        details = {\n",
    "            'description': snippet.get('description', ''),\n",
    "            'channel_id': snippet['channelId'],\n",
    "            'likes': stats.get('likeCount', '0'),\n",
    "            'views': stats.get('viewCount', '0'),\n",
    "            'comments': stats.get('commentCount', '0'),\n",
    "            'stages': snippet.get('liveBroadcastContent', ''),\n",
    "            'category': snippet.get('categoryId', ''),\n",
    "            'licensed_content': content_details.get('licensedContent', False),\n",
    "            'duration': duration,\n",
    "            'comments_enabled': 'commentCount' in stats\n",
    "        }\n",
    "    return details\n",
    "\n",
    "def parse_duration(duration):\n",
    "    import isodate\n",
    "    duration = isodate.parse_duration(duration)\n",
    "    return duration.total_seconds() / 60  # Convert to minutes\n",
    "\n",
    "def get_top_comments(api_key, video_id):\n",
    "    url = f\"https://www.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId={video_id}&key={api_key}&order=relevance&maxResults=3\"\n",
    "    response = requests.get(url).json()\n",
    "    top_comments = []\n",
    "    if 'items' in response:\n",
    "        for item in response['items']:\n",
    "            top_comments.append(item['snippet']['topLevelComment']['snippet']['textDisplay'])\n",
    "    return top_comments\n",
    "\n",
    "def collect_videos(api_key, search_query, order, max_videos):\n",
    "    collected_data = []\n",
    "    next_page_token = None\n",
    "    while len(collected_data) < max_videos:\n",
    "        url = f\"https://www.googleapis.com/youtube/v3/search?key={api_key}&q={search_query}&part=snippet&type=video&maxResults={min(50, max_videos-len(collected_data))}&order={order}&pageToken={next_page_token or ''}\"\n",
    "        response = requests.get(url).json()\n",
    "        if \"items\" not in response:\n",
    "            break\n",
    "        \n",
    "        for item in response[\"items\"]:\n",
    "            video_id = item[\"id\"][\"videoId\"]\n",
    "            video_details = get_video_details(api_key, video_id)\n",
    "            duration = video_details.get('duration', 0)\n",
    "            if 4 <= duration <= 20:  # Example duration filter in minutes\n",
    "                channel_details = get_channel_details(api_key, video_details['channel_id'])\n",
    "                top_comments = get_top_comments(api_key, video_id)\n",
    "                \n",
    "                collected_data.append({\n",
    "                    \"search_query\": search_query,\n",
    "                    \"video_id\": video_id,\n",
    "                    \"title\": item[\"snippet\"][\"title\"],\n",
    "                    \"video_url\": f\"https://www.youtube.com/watch?v={video_id}\",\n",
    "                    \"description\": video_details['description'],\n",
    "                    \"channel_id\": video_details['channel_id'],\n",
    "                    \"subscriber_count\": channel_details['subscriber_count'],\n",
    "                    \"likes\": video_details['likes'],\n",
    "                    \"views\": video_details['views'],\n",
    "                    \"comments\": video_details['comments'],\n",
    "                    \"top_comments\": top_comments,\n",
    "                    \"stages\": video_details['stages'],\n",
    "                    \"category\": video_details['category'],\n",
    "                    \"licensed_content\": video_details['licensed_content'],\n",
    "                    \"duration\": video_details['duration'],\n",
    "                    \"comments_enabled\": video_details['comments_enabled'],\n",
    "                    \"order\": order\n",
    "                })\n",
    "                \n",
    "                if len(collected_data) >= max_videos:\n",
    "                    break\n",
    "        \n",
    "        next_page_token = response.get(\"nextPageToken\")\n",
    "        if not next_page_token:\n",
    "            break\n",
    "    \n",
    "    return collected_data\n",
    "\n",
    "# Your API Key\n",
    "api_key = \"AIzaSyCGxK6OOPUk7DCODbiFy2jZ27bF3Mlxgu4\"\n",
    "\n",
    "# Define the search query and video count\n",
    "search_query = \"black women disparities medicine\"\n",
    "videos_per_type = 50  # Number of videos per relevance and recent type (total 100)\n",
    "\n",
    "# Placeholder for collected video data\n",
    "video_data = []\n",
    "\n",
    "# Collect relevance videos\n",
    "video_data += collect_videos(api_key, search_query, 'relevance', videos_per_type)\n",
    "# Collect recent videos\n",
    "video_data += collect_videos(api_key, search_query, 'date', videos_per_type)\n",
    "\n",
    "# Save video data to a CSV file\n",
    "df = pd.DataFrame(video_data)\n",
    "df.to_csv('black_women_disparities_medicine_videos.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ebb39fd-2a4c-4189-87fe-43c2efd37703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\al098\\anaconda3\\lib\\site-packages (2.32.2)\n",
      "Collecting isodate\n",
      "  Downloading isodate-0.6.1-py2.py3-none-any.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\al098\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\al098\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\al098\\anaconda3\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\al098\\anaconda3\\lib\\site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\al098\\anaconda3\\lib\\site-packages (from requests) (2024.6.2)\n",
      "Requirement already satisfied: six in c:\\users\\al098\\anaconda3\\lib\\site-packages (from isodate) (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\al098\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\al098\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\al098\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\al098\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "   ---------------------------------------- 0.0/41.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/41.7 kB ? eta -:--:--\n",
      "   --------- ------------------------------ 10.2/41.7 kB ? eta -:--:--\n",
      "   ----------------------------- ---------- 30.7/41.7 kB 325.1 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 41.7/41.7 kB 334.9 kB/s eta 0:00:00\n",
      "Installing collected packages: isodate\n",
      "Successfully installed isodate-0.6.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests isodate pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e32e2c27-5d14-4167-b430-88760d05aaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\al098\\anaconda3\\lib\\site-packages (2.32.2)\n",
      "Requirement already satisfied: isodate in c:\\users\\al098\\anaconda3\\lib\\site-packages (0.6.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\al098\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\al098\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\al098\\anaconda3\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\al098\\anaconda3\\lib\\site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\al098\\anaconda3\\lib\\site-packages (from requests) (2024.6.2)\n",
      "Requirement already satisfied: six in c:\\users\\al098\\anaconda3\\lib\\site-packages (from isodate) (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\al098\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\al098\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\al098\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\al098\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests isodate pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cec3306-51bb-4b5f-b29b-cd84fef0e241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import isodate\n",
    "\n",
    "def get_channel_details(api_key, channel_id):\n",
    "    url = f\"https://www.googleapis.com/youtube/v3/channels?part=statistics&id={channel_id}&key={api_key}\"\n",
    "    response = requests.get(url).json()\n",
    "    if 'items' in response and len(response['items']) > 0:\n",
    "        stats = response['items'][0]['statistics']\n",
    "        return {\n",
    "            'subscriber_count': stats.get('subscriberCount', '0')\n",
    "        }\n",
    "    return {'subscriber_count': '0'}\n",
    "\n",
    "def get_video_details(api_key, video_id):\n",
    "    url = f\"https://www.googleapis.com/youtube/v3/videos?part=statistics,snippet,contentDetails&id={video_id}&key={api_key}\"\n",
    "    response = requests.get(url).json()\n",
    "    details = {}\n",
    "    if 'items' in response and len(response['items']) > 0:\n",
    "        stats = response['items'][0]['statistics']\n",
    "        snippet = response['items'][0]['snippet']\n",
    "        content_details = response['items'][0]['contentDetails']\n",
    "        duration = parse_duration(content_details['duration'])\n",
    "        details = {\n",
    "            'description': snippet.get('description', ''),\n",
    "            'channel_id': snippet['channelId'],\n",
    "            'likes': stats.get('likeCount', '0'),\n",
    "            'views': stats.get('viewCount', '0'),\n",
    "            'comments': stats.get('commentCount', '0'),\n",
    "            'stages': snippet.get('liveBroadcastContent', ''),\n",
    "            'category': snippet.get('categoryId', ''),\n",
    "            'licensed_content': content_details.get('licensedContent', False),\n",
    "            'duration': duration,\n",
    "            'comments_enabled': 'commentCount' in stats\n",
    "        }\n",
    "    return details\n",
    "\n",
    "def parse_duration(duration):\n",
    "    duration = isodate.parse_duration(duration)\n",
    "    return duration.total_seconds() / 60  # Convert to minutes\n",
    "\n",
    "def get_top_comments(api_key, video_id):\n",
    "    url = f\"https://www.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId={video_id}&key={api_key}&order=relevance&maxResults=3\"\n",
    "    response = requests.get(url).json()\n",
    "    top_comments = []\n",
    "    if 'items' in response:\n",
    "        for item in response['items']:\n",
    "            top_comments.append(item['snippet']['topLevelComment']['snippet']['textDisplay'])\n",
    "    return top_comments\n",
    "\n",
    "def collect_videos(api_key, search_query, order, max_videos):\n",
    "    collected_data = []\n",
    "    next_page_token = None\n",
    "    while len(collected_data) < max_videos:\n",
    "        url = f\"https://www.googleapis.com/youtube/v3/search?key={api_key}&q={search_query}&part=snippet&type=video&maxResults={min(50, max_videos-len(collected_data))}&order={order}&pageToken={next_page_token or ''}\"\n",
    "        response = requests.get(url).json()\n",
    "        if \"items\" not in response:\n",
    "            break\n",
    "        \n",
    "        for item in response[\"items\"]:\n",
    "            video_id = item[\"id\"][\"videoId\"]\n",
    "            video_details = get_video_details(api_key, video_id)\n",
    "            duration = video_details.get('duration', 0)\n",
    "            if 4 <= duration <= 20:  # Example duration filter in minutes\n",
    "                channel_details = get_channel_details(api_key, video_details['channel_id'])\n",
    "                top_comments = get_top_comments(api_key, video_id)\n",
    "                \n",
    "                collected_data.append({\n",
    "                    \"search_query\": search_query,\n",
    "                    \"video_id\": video_id,\n",
    "                    \"title\": item[\"snippet\"][\"title\"],\n",
    "                    \"video_url\": f\"https://www.youtube.com/watch?v={video_id}\",\n",
    "                    \"description\": video_details['description'],\n",
    "                    \"channel_id\": video_details['channel_id'],\n",
    "                    \"subscriber_count\": channel_details['subscriber_count'],\n",
    "                    \"likes\": video_details['likes'],\n",
    "                    \"views\": video_details['views'],\n",
    "                    \"comments\": video_details['comments'],\n",
    "                    \"top_comments\": top_comments,\n",
    "                    \"stages\": video_details['stages'],\n",
    "                    \"category\": video_details['category'],\n",
    "                    \"licensed_content\": video_details['licensed_content'],\n",
    "                    \"duration\": video_details['duration'],\n",
    "                    \"comments_enabled\": video_details['comments_enabled'],\n",
    "                    \"order\": order\n",
    "                })\n",
    "                \n",
    "                if len(collected_data) >= max_videos:\n",
    "                    break\n",
    "        \n",
    "        next_page_token = response.get(\"nextPageToken\")\n",
    "        if not next_page_token:\n",
    "            break\n",
    "    \n",
    "    return collected_data\n",
    "\n",
    "# Your API Key\n",
    "api_key = \"AIzaSyCGxK6OOPUk7DCODbiFy2jZ27bF3Mlxgu4\"\n",
    "\n",
    "# Define the search query and video count\n",
    "search_query = \"black women disparities medicine\"\n",
    "videos_per_type = 50  # Number of videos per relevance and recent type (total 100)\n",
    "\n",
    "# Placeholder for collected video data\n",
    "video_data = []\n",
    "\n",
    "# Collect relevance videos\n",
    "video_data += collect_videos(api_key, search_query, 'relevance', videos_per_type)\n",
    "# Collect recent videos\n",
    "video_data += collect_videos(api_key, search_query, 'date', videos_per_type)\n",
    "\n",
    "# Save video data to a CSV file\n",
    "df = pd.DataFrame(video_data)\n",
    "df.to_csv('black_women_disparities_medicine_videos.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5617eb14-9d40-46bd-97fb-002f0054715a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
